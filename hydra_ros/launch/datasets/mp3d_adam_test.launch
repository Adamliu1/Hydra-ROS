<launch>
    <arg name="sim_time_required" default="true"/>
    <param name="use_sim_time" value="$(arg sim_time_required)"/>

    <!-- NOTE: use habitat as render backend and frontend -->
    <arg name="config_dir" default="$(find hydra)/config/habitat"/>
    <arg name="use_gt_frame" default="true" doc="use simulator-provided tf frame"/>
    <arg name="use_gt_semantics" default="true" doc="use simulator-provider semantics"/>
    <arg name="use_static_tfs" default="true" doc="publish static tfs from file"/>
    <!-- NOTE: this is not used -->
    <arg name="color_mesh_by_label" default="false" doc="display mesh colored by semantic label"/>
    <arg name="use_oriented_bounding_boxes" default="false"/>
    <arg name="use_2d_places" default="true"/>
    <arg name="use_single_channel_label_image" default="false"/>

    <!-- NOTE: this is to align with CMU dev low level navigation -->
    
    <arg name="robot_frame" default="vehicle" doc="robot body tf frame, used as the central reference point for robot components"/>
    <arg name="odom_frame" default="map" doc="odometry (map) frame, used as the starting point for navigation and localization"/>
    <arg name="sensor_frame" default="habitat_camera" doc="primary sensor frame, used for initial state estimations and primary sensor data"/>
    
    <arg name="verbosity" default="0"/>

    <!-- ADAM MODIFED HERE -->
    <arg name="use_zmq_interface" default="false"/>
    <!-- <arg name="zmq_ip" default="127.0.0.1"/>
    <arg name="zmq_send_url" default="tcp://$(arg zmq_ip):1641"/>
    <arg name="zmq_recv_url" default="tcp://$(arg zmq_ip):1642"/> -->

    <!-- semantics -->
    <arg name="labelspace_name" default="mpcat40" doc="semantic label space"/>
    <arg name="semantic_map_dir" default="$(find hydra_ros)/config/color" if="$(arg use_gt_semantics)"/>
    <arg name="semantic_map_path" default="$(arg semantic_map_dir)/$(arg labelspace_name).csv"/>

    <!-- ADAM: changed to match habitat frontend render topics -->
    <arg name="rgb_topic" default="/habitat_camera/color/image"/>
    <arg name="rgb_info_topic" default="/habitat_camera/color/camera_info"/>
    <arg name="depth_topic" default="/habitat_camera/depth/image"/>
    <!-- <arg name="label_topic" default="/habitat_camera/semantic/image" /> -->
    <arg name="label_topic" default="/habitat_camera/color/image" />

    <arg name="debug" default="false"/>

    <include file="$(find hydra_ros)/launch/static_tfs/cmu_adam_fake_car.xml" if="$(arg use_static_tfs)"/>

    <!-- Static Link, connecting map to world frame, also convert habitat frame to sensor frame -->
    <node pkg="tf2_ros" type="static_transform_publisher" name="sensor_to_habitat" args="0 0 0 0.5 -0.5 0.5 -0.5 sensor habitat_camera" if="$(arg use_gt_frame)"/>
    <node pkg="tf2_ros" type="static_transform_publisher" name="fake_world_tf" args="0 0 0 0 0 0 1 map world" if="$(arg use_gt_frame)"/>

    <include file="$(find semantic_inference_ros)/launch/semantic_inference.launch"
             pass_all_args="true"
             unless="$(arg use_gt_semantics)">
        <arg name="enable_pointcloud" value="false"/>
    </include>

    <include file="$(find hydra_ros)/launch/hydra.launch" pass_all_args="true">
        <arg name="robot_id" default="0"/>
        <arg name="dataset_name" default="mp3d"/>
        <arg name="rviz_dir" default="$(find hydra_ros)/rviz"/>
        <arg name="rviz_file" default="uhumans2.rviz"/>
    </include>
    
</launch>
